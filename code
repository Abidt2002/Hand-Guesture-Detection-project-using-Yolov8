from google.colab import drive
drive.mount('/content/drive')
!unzip "/content/drive/MyDrive/Hand Gestures Datset.zip" -d "/content/hand_gesture_dataset"
!pip install pandas ultralytics opencv-python
import os
import pandas as pd
import cv2

def convert_annotations_csv_to_yolo(folder):
    csv_file = os.path.join(folder, "_annotations.csv")
    image_dir = os.path.join(folder)
    label_dir = os.path.join(folder, "labels")
    os.makedirs(label_dir, exist_ok=True)

    df = pd.read_csv(csv_file)

    for img_name in df['filename'].unique():
        img_path = os.path.join(image_dir, img_name)
        if not os.path.exists(img_path):
            print("Image not found:", img_path)
            continue

        img = cv2.imread(img_path)
        if img is None:
            print("Failed to read image:", img_path)
            continue

        h, w, _ = img.shape
        label_path = os.path.join(label_dir, img_name.replace('.jpg', '.txt'))

        content = ""
        for _, row in df[df['filename'] == img_name].iterrows():
            x_center = ((row['xmin'] + row['xmax']) / 2) / w
            y_center = ((row['ymin'] + row['ymax']) / 2) / h
            width = (row['xmax'] - row['xmin']) / w
            height = (row['ymax'] - row['ymin']) / h
            class_id = int(row['class'])  # Assuming class is ID (0,1,...)
            content += f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"

        with open(label_path, "w") as f:
            f.write(content.strip())
base = "/content/hand_gesture_dataset"
convert_annotations_csv_to_yolo(os.path.join(base, "train"))
convert_annotations_csv_to_yolo(os.path.join(base, "valid"))
convert_annotations_csv_to_yolo(os.path.join(base, "test"))
import shutil
for split in ['train', 'valid', 'test']:
    folder = os.path.join(base, split)
    os.makedirs(os.path.join(folder, 'images'), exist_ok=True)
    os.makedirs(os.path.join(folder, 'labels'), exist_ok=True)

    # Move images to images/
    for file in os.listdir(folder):
        if file.endswith(".jpg"):
            shutil.move(os.path.join(folder, file), os.path.join(folder, 'images', file))
yaml_text = """
path: /content/hand_gesture_dataset
train: train/images
val: valid/images
test: test/images

nc: 13
names: ['thumbs_up', 'peace', 'stop', 'fist', 'open_hand', 'call_me', 'ok', 'rock', 'palm', 'point_up', 'two_fingers', 'three_fingers', 'four_fingers']
"""

with open("gesture_data.yaml", "w") as f:
    f.write(yaml_text)
from ultralytics import YOLO
model = YOLO("yolov8n.pt")
model.train(data="gesture_data.yaml", epochs=30, imgsz=640)
model.val()
# Inference on test set
model.predict(source="/content/hand_gesture_dataset/test/images", save=True, conf=0.4)

